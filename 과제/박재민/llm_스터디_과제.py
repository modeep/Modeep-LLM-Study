# -*- coding: utf-8 -*-
"""LLM 스터디 과제

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V3MQiBykUGM8FSChqv9uLkaHuYLXmVZG
"""

!pip install transformers datasets huggingface_hub

from transformers import AutoModel, AutoTokenizer
model_id = "klue/bert-base"
model = AutoModel.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)



from datasets import load_dataset

imdb_ds = load_dataset("yangwang825/klue-ynat")

imdb_ds["train"][0]

train_ds = imdb_ds["train"]
test_ds = imdb_ds["validation"]

import torch
import numpy as np
from transformers import (
    Trainer,
    TrainingArguments,
    AutoModelForSequenceClassification,
    AutoTokenizer
)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


model_id = "klue/bert-base"
model = AutoModelForSequenceClassification.from_pretrained(
    model_id, num_labels=len(train_ds.features['label'].names)
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

train_ds = train_ds.map(tokenize_function, batched=True)
test_ds = test_ds.map(tokenize_function, batched=True)

import os
os.environ["WANDB_DISABLED"] = "true"

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_strategy="epoch",
    learning_rate=5e-5,
    push_to_hub=False,
    report_to="none"
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {"accuracy": (predictions == labels).mean()}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    processing_class=tokenizer,
    compute_metrics=compute_metrics,
)

print(trainer.evaluate(test_ds))

trainer.train()

print(trainer.evaluate(test_ds))